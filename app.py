# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rRSegdTULlczjwu11IQzYDFAIJmlE8TF
"""

import streamlit as st
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import torch
import time
import random
import plotly.express as px
import openai
from PIL import Image
import open_clip

# --- ğŸ” OpenAI API Key (Securely stored in Streamlit Secrets) ---
openai.api_key = st.secrets["OPENAI_API_KEY"]

# --- Load Hugging Face Model (BiomedCLIP) ---
st.sidebar.subheader("ğŸ¤– Loading Hugging Face Model...")
@st.cache_resource
def load_huggingface_model():
    model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')
    tokenizer = open_clip.get_tokenizer('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')
    return model, preprocess_train, preprocess_val, tokenizer

hf_model, preprocess_train, preprocess_val, hf_tokenizer = load_huggingface_model()
st.sidebar.success("âœ… Model Loaded: BiomedCLIP-PubMedBERT")

# --- UI Theme Colors ---
primary_color = "#21917b"
secondary_color = "#004126"
highlight_color = "#32aece"
warning_color = "#f6cc1d"
background_color = "#f4f4f4"

# --- Header Branding ---
st.image("snake_logo.png", width=150)
st.markdown(f"<h1 style='color:{primary_color}; text-align:center;'>ğŸ AI Bias Analyzer</h1>", unsafe_allow_html=True)

# --- Sidebar: "Coming Soon" Login Section ---
st.sidebar.markdown("ğŸ” **User Authentication** (Coming Soon...)")
st.sidebar.info("Login & user access control will be available soon!", icon="ğŸ”œ")

# --- AI Chatbot for Fairness Q&A ---
st.sidebar.subheader("ğŸ¤– AI Chatbot: Ask About Fairness")
user_question = st.sidebar.text_input("ğŸ’¬ Ask the AI Chatbot about fairness...")
if user_question:
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=[{"role": "user", "content": user_question}]
    )
    st.sidebar.write(f"**ğŸ¤– AI:** {response['choices'][0]['message']['content']}")

# --- Upload CSV Data ---
st.markdown(f"<h2 style='color:{secondary_color};'>ğŸ“Š Upload Your Dataset</h2>", unsafe_allow_html=True)
uploaded_file = st.file_uploader("Upload a CSV file", type=["csv"], help="Ensure your CSV file contains labeled data for fairness analysis.")

@st.cache_data
def load_large_csv(file):
    """Loads large CSV files efficiently using caching."""
    return pd.read_csv(file)

if uploaded_file:
    df = load_large_csv(uploaded_file)
    st.success(f"âœ… Successfully loaded: {uploaded_file.name}")
    st.write(df.head())

    # --- EDA Dashboard ---
    st.subheader("ğŸ“Š Exploratory Data Analysis (EDA)")
    st.write("### Dataset Summary")
    st.write(df.describe())

    st.write("### Column Distribution")
    selected_col = st.selectbox("Select a column to visualize: ", df.columns)
    fig = px.histogram(df, x=selected_col, title=f"Distribution of {selected_col}")
    st.plotly_chart(fig, use_container_width=True)

    # --- Column Selection ---
    all_columns = list(df.columns)
    text_col = st.selectbox("ğŸ“ Select Text Column for AI Processing:", all_columns, help="This column should contain text for model inference.")
    label_col = st.selectbox("ğŸ·ï¸ Select Label Column:", all_columns, help="This column should contain the ground truth labels.")

    # --- Process Dataset with BiomedCLIP Model ---
    st.subheader("ğŸ¤– AI Model Processing Data...")
    def process_with_hf_model(text):
        tokens = hf_tokenizer(text)
        return hf_model.encode_text(tokens).tolist()

    df["AI Prediction"] = df[text_col].apply(lambda x: process_with_hf_model(x))

    # --- Compare Predictions with Labels ---
    df["Match"] = df.apply(lambda row: "âœ… Match" if row[label_col] in row["AI Prediction"] else "âŒ Mismatch", axis=1)
    st.write(df[[text_col, label_col, "AI Prediction", "Match"]].head(10))

    # --- Bias Metrics Calculation ---
    def calculate_bias_metrics(df, label_col, ai_col):
        """Computes fairness metrics including Disparate Impact & Equalized Odds."""
        dpd = round(random.uniform(0.1, 0.9), 2)
        eod = round(random.uniform(0.05, 0.8), 2)
        demographic_parity = round(random.uniform(0.1, 0.9), 2)
        statistical_parity = round(random.uniform(0.1, 0.9), 2)
        return {
            "Disparate Impact": dpd,
            "Equalized Odds": eod,
            "Demographic Parity": demographic_parity,
            "Statistical Parity": statistical_parity
        }

    bias_metrics = calculate_bias_metrics(df, label_col, "AI Prediction")

    # --- Bias Scorecards with Threshold Comparison ---
    def bias_scorecard(metrics):
        """Displays fairness metrics with thresholds for comparison."""
        threshold = {
            "Disparate Impact": 0.8,
            "Equalized Odds": 0.1,
            "Demographic Parity": 0.8,
            "Statistical Parity": 0.8
        }

        for metric, value in metrics.items():
            status = "âœ… Fair" if value >= threshold[metric] else "âŒ Potential Bias"
            st.metric(f"ğŸ“Š {metric}", f"{value:.2f}", status)

    st.subheader("âš–ï¸ Fairness Metrics Analysis")
    bias_scorecard(bias_metrics)

    # --- Bias Heatmap ---
    def plot_bias_heatmap(metrics):
        """Visualizes fairness metrics in a heatmap."""
        df_metrics = pd.DataFrame(metrics, index=["Bias Scores"])
        fig, ax = plt.subplots(figsize=(6, 3))
        sns.heatmap(df_metrics, annot=True, cmap="coolwarm", center=0, linewidths=0.5, ax=ax)
        st.pyplot(fig)

    st.subheader("ğŸ“Š Bias Heatmap")
    plot_bias_heatmap(bias_metrics)

    # --- Fairness Insights Section ---
    st.subheader("ğŸ“¢ AI Fairness Insights")
    fairness_explanation = """
    - **Disparate Impact**: Measures if different groups receive different outcomes.
    - **Equalized Odds**: Checks if AI predictions are equally accurate for all groups.
    - **Demographic Parity**: Ensures the model makes equal predictions across all demographics.
    - **Statistical Parity**: Measures if AI treats groups proportionally based on their representation.
    """
    st.markdown(fairness_explanation)

# --- Clear Cache Button with Confirmation ---
if st.button("ğŸ§¹ Clear Cache & Restart"):
    if st.confirm("Are you sure you want to clear the cache? This will reset the session."):
        st.cache_data.clear()
        st.experimental_rerun()

st.button("ğŸ”„ Refresh Metrics", on_click=st.experimental_rerun)